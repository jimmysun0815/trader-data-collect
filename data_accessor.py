"""
Polymarketæ•°æ®è®¿é—®ä»£ç†å±‚

åŠŸèƒ½ï¼š
1. ç»Ÿä¸€è®¿é—®çƒ­æ•°æ®ï¼ˆSSHFSæŒ‚è½½çš„real_hot/ï¼‰å’Œå†·æ•°æ®ï¼ˆå½’æ¡£çš„real_archive/ï¼‰
2. è‡ªåŠ¨è·¯ç”±æ•°æ®è¯·æ±‚åˆ°æ­£ç¡®çš„å­˜å‚¨ä½ç½®
3. æŒ‰éœ€æ‹‰å–å†·æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜
4. LRUç¼“å­˜ç®¡ç†ï¼ˆé™åˆ¶æœ¬åœ°ç¼“å­˜å¤§å°ï¼‰
5. æ™ºèƒ½é¢„å–ç›¸é‚»çª—å£æ•°æ®

ç”¨æ³•ç¤ºä¾‹ï¼š
    from data_accessor import DataAccessor
    
    # åˆå§‹åŒ–ï¼ˆè‡ªåŠ¨æ£€æµ‹è·¯å¾„ï¼‰
    accessor = DataAccessor()
    
    # è·å–å•ä¸ªçª—å£æ•°æ®æ–‡ä»¶è·¯å¾„
    path = accessor.get_window_jsonl(1767507300)
    with open(path) as f:
        for line in f:
            data = json.loads(line)
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çª—å£
    windows = accessor.list_all_windows()
    
    # æ‰¹é‡è·å–çª—å£ï¼ˆè‡ªåŠ¨é¢„å–ï¼‰
    for ws in windows:
        path = accessor.get_window_jsonl(ws)
        # å¤„ç†æ•°æ®...

æˆ–è€…ä½¿ç”¨ä¾¿æ·å‡½æ•°è‡ªåŠ¨è®¾ç½®å…¨å±€è·¯å¾„æ˜ å°„ï¼š
    from data_accessor import setup_data_paths
    setup_data_paths()
    
    # ä¹‹åæ‰€æœ‰å¯¹ real/ çš„è®¿é—®ä¼šè‡ªåŠ¨è·¯ç”±åˆ° real_hot + ç¼“å­˜
"""

from __future__ import annotations

import glob as _builtin_glob
import json
import os
import re
import subprocess
import time
from collections import OrderedDict
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterator, Optional


class DataAccessor:
    """æ•°æ®è®¿é—®ä»£ç†"""
    
    def __init__(
        self,
        hot_dir: Optional[Path] = None,
        cache_dir: Optional[Path] = None,
        vps_user: Optional[str] = None,
        vps_host: Optional[str] = None,
        vps_archive_path: Optional[str] = None,
        cache_max_size_mb: int = 1024,
    ):
        """
        åˆå§‹åŒ–æ•°æ®è®¿é—®å™¨
        
        å‚æ•°ï¼š
            hot_dir: æœ¬åœ°çƒ­æ•°æ®ç›®å½•ï¼ˆSSHFSæŒ‚è½½ç‚¹ï¼‰ï¼Œé»˜è®¤ ~/Desktop/workspace/polymarket/real_hot
            cache_dir: æœ¬åœ°å†·æ•°æ®ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ ~/Desktop/workspace/polymarket/real_cache
            vps_user: VPSç”¨æˆ·åï¼ˆä»ç¯å¢ƒå˜é‡VPS_USERè¯»å–ï¼‰
            vps_host: VPSåœ°å€ï¼ˆä»ç¯å¢ƒå˜é‡VPS_HOSTè¯»å–ï¼‰
            vps_archive_path: VPSå½’æ¡£ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ ~/polymarket/real_archiveï¼‰
            cache_max_size_mb: æœ¬åœ°ç¼“å­˜æœ€å¤§å¤§å°ï¼ˆMBï¼‰
        """
        # è®¾ç½®ç›®å½•
        workspace = Path.home() / "Desktop" / "workspace" / "polymarket"
        self.hot_dir = hot_dir or (workspace / "real_hot")
        self.cache_dir = cache_dir or (workspace / "real_cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # VPSé…ç½®
        self.vps_user = vps_user or os.environ.get("VPS_USER")
        self.vps_host = vps_host or os.environ.get("VPS_HOST")
        self.vps_archive_path = vps_archive_path or "~/polymarket/real_archive"
        
        # ç¼“å­˜é…ç½®
        self.cache_max_size_bytes = cache_max_size_mb * 1024 * 1024
        self.cache_access_log: OrderedDict[str, float] = OrderedDict()  # filename -> last_access_time
        
        # é¢„å–é…ç½®
        self.prefetch_enabled = True
        self.prefetch_count = 3  # é¢„å–ç›¸é‚»çª—å£æ•°é‡
        
    def parse_window_start_from_filename(self, filename: str) -> Optional[int]:
        """ä»æ–‡ä»¶åæå–window_startæ—¶é—´æˆ³"""
        m = re.match(r"btc-updown-15m-(\d+)_", filename)
        if m:
            try:
                return int(m.group(1))
            except Exception:
                return None
        return None
    
    def get_archive_subpath(self, window_start: int) -> str:
        """è®¡ç®—å½’æ¡£å­è·¯å¾„ï¼ˆå¹´æœˆç›®å½•ï¼‰"""
        dt = datetime.fromtimestamp(window_start, tz=timezone.utc)
        return dt.strftime("%Y-%m")
    
    def get_window_jsonl(self, window_start: int, prefetch: bool = True) -> Path:
        """
        è·å–æŒ‡å®šçª—å£çš„JSONLæ–‡ä»¶è·¯å¾„
        
        è¿”å›æœ¬åœ°å¯è®¿é—®çš„æ–‡ä»¶è·¯å¾„ï¼ˆçƒ­æ•°æ®/ç¼“å­˜/æ–°æ‹‰å–ï¼‰
        """
        # 1. å…ˆæŸ¥çƒ­æ•°æ®ï¼ˆSSHFSæŒ‚è½½ï¼‰
        pattern = f"btc-updown-15m-{window_start}_*.jsonl"
        hot_matches = list(self.hot_dir.glob(pattern))
        if hot_matches:
            hot_path = hot_matches[0]
            # æ›´æ–°è®¿é—®æ—¥å¿—
            self._update_access_log(hot_path.name)
            
            # è§¦å‘é¢„å–
            if prefetch and self.prefetch_enabled:
                self._prefetch_neighbors(window_start)
            
            return hot_path
        
        # 2. æŸ¥æœ¬åœ°ç¼“å­˜
        year_month = self.get_archive_subpath(window_start)
        cache_month_dir = self.cache_dir / year_month
        
        if cache_month_dir.exists():
            cache_matches = list(cache_month_dir.glob(pattern))
            if cache_matches:
                cache_path = cache_matches[0]
                # æ›´æ–°è®¿é—®æ—¥å¿—
                self._update_access_log(cache_path.name)
                
                # è§¦å‘é¢„å–
                if prefetch and self.prefetch_enabled:
                    self._prefetch_neighbors(window_start)
                
                return cache_path
        
        # 3. ä»VPSæ‹‰å–åˆ°ç¼“å­˜
        print(f"  ğŸŒ ä»VPSæ‹‰å–å†·æ•°æ®: window_start={window_start}")
        fetched_path = self._fetch_from_vps(window_start, year_month)
        
        if fetched_path:
            # è§¦å‘é¢„å–
            if prefetch and self.prefetch_enabled:
                self._prefetch_neighbors(window_start)
            
            return fetched_path
        
        raise FileNotFoundError(
            f"æ— æ³•æ‰¾åˆ°çª—å£æ•°æ®: window_start={window_start}, "
            f"å·²æŸ¥æ‰¾: çƒ­æ•°æ®({self.hot_dir}), ç¼“å­˜({cache_month_dir}), VPSå½’æ¡£"
        )
    
    def _fetch_from_vps(self, window_start: int, year_month: str) -> Optional[Path]:
        """ä»VPSæ‹‰å–å½’æ¡£æ–‡ä»¶åˆ°æœ¬åœ°ç¼“å­˜"""
        if not self.vps_user or not self.vps_host:
            print(f"  âš ï¸  æœªé…ç½®VPSè¿æ¥ä¿¡æ¯ï¼ˆVPS_USER, VPS_HOSTï¼‰ï¼Œæ— æ³•æ‹‰å–å†·æ•°æ®")
            return None
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        cache_month_dir = self.cache_dir / year_month
        cache_month_dir.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºè¿œç¨‹è·¯å¾„pattern
        pattern = f"btc-updown-15m-{window_start}_*.jsonl*"
        remote_path = f"{self.vps_archive_path}/{year_month}/{pattern}"
        
        try:
            # å…ˆåˆ—å‡ºè¿œç¨‹æ–‡ä»¶
            remote_spec = f"{self.vps_user}@{self.vps_host}:{remote_path}"
            result = subprocess.run(
                ["ssh", f"{self.vps_user}@{self.vps_host}", f"ls {remote_path}"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            
            if result.returncode != 0:
                print(f"  âš ï¸  è¿œç¨‹æ–‡ä»¶ä¸å­˜åœ¨: {remote_path}")
                return None
            
            remote_files = result.stdout.strip().split("\n")
            if not remote_files or not remote_files[0]:
                print(f"  âš ï¸  è¿œç¨‹æ–‡ä»¶åˆ—è¡¨ä¸ºç©º")
                return None
            
            # æ‹‰å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
            remote_file = remote_files[0]
            filename = Path(remote_file).name
            local_path = cache_month_dir / filename
            
            # ä½¿ç”¨scpæ‹‰å–
            remote_full = f"{self.vps_user}@{self.vps_host}:{remote_file}"
            result = subprocess.run(
                ["scp", "-q", remote_full, str(local_path)],
                capture_output=True,
                timeout=60,
            )
            
            if result.returncode != 0:
                print(f"  âŒ SCPæ‹‰å–å¤±è´¥: {result.stderr.decode()}")
                return None
            
            # å¦‚æœæ˜¯å‹ç¼©æ–‡ä»¶ï¼Œè§£å‹
            if filename.endswith(".gz"):
                print(f"  ğŸ“¦ è§£å‹æ–‡ä»¶: {filename}")
                import gzip
                import shutil
                
                uncompressed_path = cache_month_dir / filename[:-3]
                with gzip.open(local_path, "rb") as f_in:
                    with open(uncompressed_path, "wb") as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                # åˆ é™¤å‹ç¼©æ–‡ä»¶
                local_path.unlink()
                local_path = uncompressed_path
            
            print(f"  âœ“ å·²æ‹‰å–åˆ°ç¼“å­˜: {local_path.name}")
            
            # æ›´æ–°è®¿é—®æ—¥å¿—
            self._update_access_log(local_path.name)
            
            # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œå¿…è¦æ—¶æ¸…ç†
            self._cleanup_cache_if_needed()
            
            return local_path
            
        except subprocess.TimeoutExpired:
            print(f"  âš ï¸  SSH/SCPè¶…æ—¶")
            return None
        except Exception as e:
            print(f"  âŒ æ‹‰å–å‡ºé”™: {e}")
            return None
    
    def _prefetch_neighbors(self, window_start: int) -> None:
        """é¢„å–ç›¸é‚»çª—å£ï¼ˆåå°å¼‚æ­¥ï¼Œä¸é˜»å¡ï¼‰"""
        # 15åˆ†é’Ÿçª—å£ï¼Œæ¯ä¸ªçª—å£900ç§’
        window_size = 900
        
        for i in range(1, self.prefetch_count + 1):
            next_window = window_start + i * window_size
            try:
                # æ£€æŸ¥æ˜¯å¦å·²åœ¨çƒ­æ•°æ®æˆ–ç¼“å­˜ä¸­
                pattern = f"btc-updown-15m-{next_window}_*.jsonl"
                if list(self.hot_dir.glob(pattern)):
                    continue
                
                year_month = self.get_archive_subpath(next_window)
                cache_month_dir = self.cache_dir / year_month
                if cache_month_dir.exists() and list(cache_month_dir.glob(pattern)):
                    continue
                
                # åå°æ‹‰å–ï¼ˆä¸æ‰“å°è¯¦ç»†æ—¥å¿—ï¼‰
                self._fetch_from_vps(next_window, year_month)
                
            except Exception:
                # é¢„å–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                pass
    
    def _update_access_log(self, filename: str) -> None:
        """æ›´æ–°æ–‡ä»¶è®¿é—®æ—¥å¿—ï¼ˆLRUï¼‰"""
        # ç§»é™¤æ—§è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if filename in self.cache_access_log:
            del self.cache_access_log[filename]
        
        # æ·»åŠ æ–°è®°å½•åˆ°æœ«å°¾
        self.cache_access_log[filename] = time.time()
    
    def _cleanup_cache_if_needed(self) -> None:
        """æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œè¶…é™æ—¶æ¸…ç†æœ€æ—§çš„æ–‡ä»¶"""
        # è®¡ç®—ç¼“å­˜æ€»å¤§å°
        total_size = 0
        file_sizes = {}
        
        for file_path in self.cache_dir.rglob("btc-updown-15m-*.jsonl*"):
            size = file_path.stat().st_size
            total_size += size
            file_sizes[file_path.name] = (file_path, size)
        
        if total_size <= self.cache_max_size_bytes:
            return  # æœªè¶…é™
        
        print(f"  ğŸ§¹ ç¼“å­˜è¶…é™ ({total_size / 1024 / 1024:.1f}MB > {self.cache_max_size_bytes / 1024 / 1024:.1f}MB)ï¼Œå¼€å§‹æ¸…ç†...")
        
        # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼ˆæœ€æ—§çš„åœ¨å‰ï¼‰
        sorted_files = sorted(
            self.cache_access_log.items(),
            key=lambda x: x[1],  # æŒ‰è®¿é—®æ—¶é—´æ’åº
        )
        
        cleaned_size = 0
        cleaned_count = 0
        
        for filename, _ in sorted_files:
            if total_size - cleaned_size <= self.cache_max_size_bytes:
                break
            
            if filename in file_sizes:
                file_path, size = file_sizes[filename]
                try:
                    file_path.unlink()
                    cleaned_size += size
                    cleaned_count += 1
                    del self.cache_access_log[filename]
                except Exception as e:
                    print(f"  âš ï¸  åˆ é™¤æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        
        print(f"  âœ“ æ¸…ç†å®Œæˆ: åˆ é™¤{cleaned_count}ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾{cleaned_size / 1024 / 1024:.1f}MB")
    
    def list_all_windows(self, use_cache: bool = True) -> list[int]:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„çª—å£æ—¶é—´æˆ³
        
        è¿”å›æ’åºåçš„window_startåˆ—è¡¨
        """
        windows = set()
        
        # 1. æ‰«æçƒ­æ•°æ®
        if self.hot_dir.exists():
            for file_path in self.hot_dir.glob("btc-updown-15m-*.jsonl"):
                ws = self.parse_window_start_from_filename(file_path.name)
                if ws:
                    windows.add(ws)
        
        # 2. æ‰«ææœ¬åœ°ç¼“å­˜
        if use_cache and self.cache_dir.exists():
            for file_path in self.cache_dir.rglob("btc-updown-15m-*.jsonl*"):
                ws = self.parse_window_start_from_filename(file_path.name)
                if ws:
                    windows.add(ws)
        
        # 3. å¯é€‰ï¼šåˆ—å‡ºVPSæ‰€æœ‰æ–‡ä»¶ï¼ˆéœ€è¦SSHè¿æ¥ï¼Œè¾ƒæ…¢ï¼‰
        # æš‚æ—¶ä¸å®ç°ï¼Œéœ€è¦æ—¶å¯ä»¥æ·»åŠ 
        
        return sorted(windows)
    
    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_size = 0
        file_count = 0
        
        for file_path in self.cache_dir.rglob("btc-updown-15m-*.jsonl*"):
            total_size += file_path.stat().st_size
            file_count += 1
        
        return {
            "cache_dir": str(self.cache_dir),
            "file_count": file_count,
            "total_size_mb": total_size / 1024 / 1024,
            "max_size_mb": self.cache_max_size_bytes / 1024 / 1024,
            "usage_percent": (total_size / self.cache_max_size_bytes * 100) if self.cache_max_size_bytes > 0 else 0,
        }


# å…¨å±€å•ä¾‹
_global_accessor: Optional[DataAccessor] = None


def get_accessor() -> DataAccessor:
    """è·å–å…¨å±€æ•°æ®è®¿é—®å™¨å®ä¾‹"""
    global _global_accessor
    if _global_accessor is None:
        _global_accessor = DataAccessor()
    return _global_accessor


def setup_data_paths() -> None:
    """
    è®¾ç½®å…¨å±€è·¯å¾„æ˜ å°„ï¼Œè®©ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨æ•°æ®ä»£ç†
    
    è°ƒç”¨æ­¤å‡½æ•°åï¼Œæ‰€æœ‰å¯¹ real/ çš„è®¿é—®ä¼šè‡ªåŠ¨è·¯ç”±åˆ° real_hot + ç¼“å­˜
    """
    accessor = get_accessor()
    
    # åˆ›å»º real ç¬¦å·é“¾æ¥æŒ‡å‘ real_hotï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    workspace = Path.home() / "Desktop" / "workspace" / "polymarket"
    real_link = workspace / "real"
    
    # å¦‚æœ real æ˜¯ç›®å½•ï¼Œé‡å‘½åä¸º real_backup
    if real_link.exists() and real_link.is_dir() and not real_link.is_symlink():
        backup_path = workspace / f"real_backup_{int(time.time())}"
        print(f"  âš ï¸  å°†åŸæœ‰ real/ ç›®å½•å¤‡ä»½åˆ°: {backup_path}")
        real_link.rename(backup_path)
    
    # åˆ›å»ºç¬¦å·é“¾æ¥
    if not real_link.exists():
        real_link.symlink_to(accessor.hot_dir)
        print(f"  âœ“ åˆ›å»ºç¬¦å·é“¾æ¥: real/ -> {accessor.hot_dir}")
    
    print(f"  âœ“ æ•°æ®è·¯å¾„å·²é…ç½®")
    print(f"     çƒ­æ•°æ®: {accessor.hot_dir}")
    print(f"     ç¼“å­˜: {accessor.cache_dir}")


# ä¾¿æ·å‡½æ•°
def get_window_jsonl(window_start: int) -> Path:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–çª—å£JSONLæ–‡ä»¶è·¯å¾„"""
    return get_accessor().get_window_jsonl(window_start)


def list_all_windows() -> list[int]:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ—å‡ºæ‰€æœ‰çª—å£"""
    return get_accessor().list_all_windows()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys
    
    accessor = DataAccessor()
    
    print("æ•°æ®è®¿é—®å™¨é…ç½®:")
    print(f"  çƒ­æ•°æ®ç›®å½•: {accessor.hot_dir}")
    print(f"  ç¼“å­˜ç›®å½•: {accessor.cache_dir}")
    print(f"  VPS: {accessor.vps_user}@{accessor.vps_host}")
    print()
    
    # åˆ—å‡ºå¯ç”¨çª—å£
    print("æ‰«æå¯ç”¨çª—å£...")
    windows = accessor.list_all_windows()
    print(f"  æ‰¾åˆ° {len(windows)} ä¸ªçª—å£")
    
    if windows:
        print(f"  æœ€æ—©: {windows[0]} ({datetime.fromtimestamp(windows[0], tz=timezone.utc)})")
        print(f"  æœ€æ–°: {windows[-1]} ({datetime.fromtimestamp(windows[-1], tz=timezone.utc)})")
    
    # ç¼“å­˜ç»Ÿè®¡
    print()
    stats = accessor.get_cache_stats()
    print("ç¼“å­˜ç»Ÿè®¡:")
    print(f"  æ–‡ä»¶æ•°: {stats['file_count']}")
    print(f"  æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
    print(f"  ä½¿ç”¨ç‡: {stats['usage_percent']:.1f}%")
    
    # æµ‹è¯•è®¿é—®
    if len(sys.argv) > 1:
        test_window = int(sys.argv[1])
        print()
        print(f"æµ‹è¯•è®¿é—®çª—å£: {test_window}")
        try:
            path = accessor.get_window_jsonl(test_window)
            print(f"  âœ“ æ–‡ä»¶è·¯å¾„: {path}")
            print(f"  æ–‡ä»¶å¤§å°: {path.stat().st_size / 1024:.1f} KB")
        except FileNotFoundError as e:
            print(f"  âŒ {e}")

