#!/usr/bin/env python3
"""
VPSè‡ªåŠ¨å½’æ¡£è„šæœ¬ - å°†è¶…è¿‡30å¤©çš„æ•°æ®ä»real_hot/è¿ç§»åˆ°real_archive/

åŠŸèƒ½ï¼š
1. æ‰«æreal_hot/ç›®å½•ä¸‹çš„btc-updown-15m-*.jsonlæ–‡ä»¶
2. è¯†åˆ«è¶…è¿‡30å¤©çš„æ–‡ä»¶ï¼ˆåŸºäºæ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³ï¼‰
3. æŒ‰æœˆå½’æ¡£åˆ°real_archive/YYYY-MM/ç›®å½•
4. å¯é€‰å‹ç¼©ï¼ˆgzipï¼‰
5. è®°å½•å½’æ¡£æ—¥å¿—

ç”¨æ³•ï¼š
  python3 archive_old_data.py [--dry-run] [--compress] [--days 30]
"""

from __future__ import annotations

import argparse
import gzip
import re
import shutil
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional


def parse_window_start_from_filename(filename: str) -> Optional[int]:
    """
    ä»æ–‡ä»¶åæå–window_startæ—¶é—´æˆ³
    ä¾‹å¦‚: btc-updown-15m-1767507300_1767507300_20260103_222417.jsonl -> 1767507300
    """
    m = re.match(r"btc-updown-15m-(\d+)_", filename)
    if m:
        try:
            return int(m.group(1))
        except Exception:
            return None
    return None


def get_archive_path(window_start: int, base_archive: Path) -> Path:
    """
    æ ¹æ®window_startè®¡ç®—å½’æ¡£è·¯å¾„
    ä¾‹å¦‚: 1767507300 -> real_archive/2026-01/
    """
    dt = datetime.fromtimestamp(window_start, tz=timezone.utc)
    year_month = dt.strftime("%Y-%m")
    return base_archive / year_month


def compress_file(src: Path, dst: Path) -> None:
    """ä½¿ç”¨gzipå‹ç¼©æ–‡ä»¶"""
    with open(src, "rb") as f_in:
        with gzip.open(dst, "wb") as f_out:
            shutil.copyfileobj(f_in, f_out)


def archive_old_files(
    hot_dir: Path,
    archive_dir: Path,
    days_threshold: int = 7,
    compress: bool = False,
    dry_run: bool = False,
) -> dict:
    """
    å½’æ¡£æ—§æ–‡ä»¶
    
    è¿”å›ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    now = time.time()
    threshold_seconds = days_threshold * 24 * 3600
    
    stats = {
        "scanned": 0,
        "archived": 0,
        "skipped": 0,
        "errors": 0,
        "size_before": 0,
        "size_after": 0,
    }
    
    # æ‰«æhotç›®å½•
    pattern = "btc-updown-15m-*.jsonl"
    files = sorted(hot_dir.glob(pattern))
    
    stats["scanned"] = len(files)
    
    for file_path in files:
        try:
            # è§£ææ–‡ä»¶åè·å–æ—¶é—´æˆ³
            window_start = parse_window_start_from_filename(file_path.name)
            if window_start is None:
                print(f"  âš ï¸  è·³è¿‡æ— æ³•è§£æçš„æ–‡ä»¶: {file_path.name}")
                stats["skipped"] += 1
                continue
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
            age_seconds = now - window_start
            if age_seconds < threshold_seconds:
                continue  # è¿˜ä¸å¤Ÿè€ï¼Œè·³è¿‡
            
            age_days = age_seconds / 86400
            
            # è®¡ç®—ç›®æ ‡è·¯å¾„
            archive_month_dir = get_archive_path(window_start, archive_dir)
            
            if compress:
                target_filename = file_path.name + ".gz"
            else:
                target_filename = file_path.name
            
            target_path = archive_month_dir / target_filename
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            stats["size_before"] += file_size
            
            if dry_run:
                print(f"  [DRY-RUN] å°†å½’æ¡£: {file_path.name} -> {target_path}")
                print(f"            å¹´é¾„: {age_days:.1f}å¤©, å¤§å°: {file_size/1024:.1f}KB")
                stats["archived"] += 1
                stats["size_after"] += file_size if not compress else int(file_size * 0.3)
                continue
            
            # åˆ›å»ºç›®æ ‡ç›®å½•
            archive_month_dir.mkdir(parents=True, exist_ok=True)
            
            # æ‰§è¡Œå½’æ¡£
            if compress:
                print(f"  ğŸ“¦ å‹ç¼©å½’æ¡£: {file_path.name} -> {target_path.relative_to(archive_dir)}")
                compress_file(file_path, target_path)
            else:
                print(f"  ğŸ“ ç§»åŠ¨å½’æ¡£: {file_path.name} -> {target_path.relative_to(archive_dir)}")
                shutil.move(str(file_path), str(target_path))
            
            target_size = target_path.stat().st_size
            stats["size_after"] += target_size
            stats["archived"] += 1
            
            compression_ratio = (target_size / file_size * 100) if compress else 100
            print(f"      å¹´é¾„: {age_days:.1f}å¤©, åŸå§‹: {file_size/1024:.1f}KB, "
                  f"å½’æ¡£: {target_size/1024:.1f}KB ({compression_ratio:.1f}%)")
            
        except Exception as e:
            print(f"  âŒ å¤„ç†æ–‡ä»¶å‡ºé”™ {file_path.name}: {e}")
            stats["errors"] += 1
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description="å½’æ¡£Polymarketæ—§æ•°æ®åˆ°æŒ‰æœˆç›®å½•"
    )
    parser.add_argument(
        "--hot-dir",
        type=Path,
        default=Path.home() / "polymarket" / "real_hot",
        help="çƒ­æ•°æ®ç›®å½•ï¼ˆé»˜è®¤: ~/polymarket/real_hotï¼‰",
    )
    parser.add_argument(
        "--archive-dir",
        type=Path,
        default=Path.home() / "polymarket" / "real_archive",
        help="å½’æ¡£ç›®å½•ï¼ˆé»˜è®¤: ~/polymarket/real_archiveï¼‰",
    )
    parser.add_argument(
        "--days",
        type=int,
        default=30,
        help="å½’æ¡£é˜ˆå€¼å¤©æ•°ï¼ˆé»˜è®¤: 30å¤©ï¼‰",
    )
    parser.add_argument(
        "--compress",
        action="store_true",
        help="ä½¿ç”¨gzipå‹ç¼©å½’æ¡£æ–‡ä»¶",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ä»…æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…ç§»åŠ¨æ–‡ä»¶",
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Polymarketæ•°æ®è‡ªåŠ¨å½’æ¡£")
    print("=" * 60)
    print(f"çƒ­æ•°æ®ç›®å½•: {args.hot_dir}")
    print(f"å½’æ¡£ç›®å½•: {args.archive_dir}")
    print(f"å½’æ¡£é˜ˆå€¼: {args.days}å¤©å‰çš„æ•°æ®")
    print(f"å‹ç¼©æ¨¡å¼: {'å¼€å¯' if args.compress else 'å…³é—­'}")
    print(f"è¿è¡Œæ¨¡å¼: {'æ¨¡æ‹Ÿè¿è¡Œï¼ˆä¸å®é™…æ“ä½œï¼‰' if args.dry_run else 'å®é™…æ‰§è¡Œ'}")
    print()
    
    # æ£€æŸ¥ç›®å½•
    if not args.hot_dir.exists():
        print(f"âŒ é”™è¯¯: çƒ­æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.hot_dir}")
        return 1
    
    # æ‰§è¡Œå½’æ¡£
    start_time = time.time()
    stats = archive_old_files(
        hot_dir=args.hot_dir,
        archive_dir=args.archive_dir,
        days_threshold=args.days,
        compress=args.compress,
        dry_run=args.dry_run,
    )
    elapsed = time.time() - start_time
    
    # æ‰“å°ç»Ÿè®¡
    print()
    print("=" * 60)
    print("å½’æ¡£ç»Ÿè®¡")
    print("=" * 60)
    print(f"æ‰«ææ–‡ä»¶æ•°: {stats['scanned']}")
    print(f"å½’æ¡£æ–‡ä»¶æ•°: {stats['archived']}")
    print(f"è·³è¿‡æ–‡ä»¶æ•°: {stats['skipped']}")
    print(f"é”™è¯¯æ•°: {stats['errors']}")
    print(f"åŸå§‹å¤§å°: {stats['size_before'] / 1024 / 1024:.2f} MB")
    print(f"å½’æ¡£å¤§å°: {stats['size_after'] / 1024 / 1024:.2f} MB")
    if stats['size_before'] > 0:
        ratio = stats['size_after'] / stats['size_before'] * 100
        print(f"å‹ç¼©æ¯”: {ratio:.1f}%")
    print(f"è€—æ—¶: {elapsed:.2f}ç§’")
    print()
    
    if args.dry_run:
        print("â„¹ï¸  è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œæœªå®é™…ç§»åŠ¨æ–‡ä»¶")
        print("   ç§»é™¤ --dry-run å‚æ•°ä»¥æ‰§è¡Œå®é™…å½’æ¡£")
    
    return 0


if __name__ == "__main__":
    exit(main())

